{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32c7efb0",
   "metadata": {},
   "source": [
    "# Topik Modeling Dengan Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9565c84",
   "metadata": {},
   "source": [
    "topic modeling merupakan suatu pendekatan untuk menganalisis kumpulan dokumen berbentuk teks dan mengelompokkan menjadi beberapa topik. Pendekatan tersebut masuk dalam pendekatan Clustering dalam studi Machine Learning. Adapun tahap-tahapnya yaitu : \n",
    "<ol>\n",
    "    <li>Crawling Data</li>\n",
    "    <li>Preprocessing Data</li>\n",
    "    <li>LSA</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b23cd2",
   "metadata": {},
   "source": [
    "## Crawling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5312c0b9",
   "metadata": {},
   "source": [
    "Crawling data adalah suatu teknik untuk mengumpulkan data secara cepat dengan menggunakan url sebagai target data yang akan dikumpulkan. Untuk mengumpulkan data bisa menggunakan berbagai tools atau library yang ada, salah satunya adalah Scrappy. Scrapy adalah framework dari python yang berspesialis dalam melakukan web scraping dalam sekala besar, untuk menggunakan scrapy pertama kita install dahulu Scrapy dengan menggunakan pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd5e660",
   "metadata": {},
   "source": [
    "### Install Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a2d27a",
   "metadata": {},
   "source": [
    "Library yang perlu diinstal untuk crawling data ada dua yaitu, Scrapy dan nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2094181",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6533454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c249cf",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa5cb3f",
   "metadata": {},
   "source": [
    "Sesudah install library yang dibutuhkan, selanjutnya kita import librarynya (untuk re sudah terinstall otomatis ketika install python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c91e8b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d831d87",
   "metadata": {},
   "source": [
    "### Melakukan Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f15b770",
   "metadata": {},
   "source": [
    "Sesudah import library yang dibutuhkan, selanjutnya melakukan tahap crawling. Disini tahap Crawl saya simpan di class QuotesSpider. Variabel start_urls berfungsi untuk menampung target url, dimana start_url akan mendapatkan data dari tahap looping \"for page in range(1,10)\". Function parse memiliki peran melakukan scrap pada element html mana, sedangkan function parse_detail memiliki peran untuk menargetkan secara spesifik seperti : \n",
    "<ul>\n",
    "    <li>Mengambil text htmlnya atau mengambil Linknya</li>\n",
    "    <li>Membuang elemen yang tidak digunakan</li>\n",
    "    <li>Mereplace kata yang tidak digunakan dengan kata yang ingin digunakan</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00bf6bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = []\n",
    "    def __init__(self):\n",
    "        url = 'https://pta.trunojoyo.ac.id/welcome/index/'\n",
    "        for page in range(1,10):\n",
    "            self.start_urls.append(url+str(page))\n",
    "\n",
    "    def parse(self, response):\n",
    "        for detail in response.css('a.gray.button::attr(href)'): \n",
    "            yield response.follow(detail.get(), callback = self.parse_detail)\n",
    "\n",
    "    def parse_detail(self, response):\n",
    "        for data in response.css('#content_journal > ul > li'):\n",
    "            yield{\n",
    "                'Judul': data.css('div:nth-child(2) > a::text').get(),\n",
    "                'Penulis': data.css('div:nth-child(2) > span::text').get().replace('Penulis : ', ''),\n",
    "                'Dospem 1': data.css('div:nth-child(3) > span::text').get().replace('Dosen Pembimbing I : ', ''),\n",
    "                'Dospem 2': data.css('div:nth-child(4) > span::text').get().replace('Dosen Pembimbing II :', ''),\n",
    "                'Abstraksi': data.css('div:nth-child(2) > p::text').get().replace('\\n\\n|\\n','').replace('ABSTRAK', ''),\n",
    "                'Abstraction': data.css('div:nth-child(4) > p::text').get().replace('\\n\\n|\\n','').replace('ABSTRACT', ''),\n",
    "                'Link Download': data.css('div:nth-child(5) > a:nth-child(1)::attr(href)').get().replace('.pdf-0.jpg', '.pdf'),\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bee6c3",
   "metadata": {},
   "source": [
    "Silahkan save codenya dan buka cmd, pastikan terbuka di folder yang ada file scrapingnya. Kemudian jalankan perintah ini di cmd untuk memproses dan menyimpan ke csv \"scrapy runspider namaFile.py -o namaFileKetikaDiSaveUlang.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5277f1e6",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8e06a9",
   "metadata": {},
   "source": [
    "Preprocessing Data adalah suatu teknik untuk merubah data mentah atau raw data menajdi informasi yang bersih dan agar bisa digunakan untuk pengolahan lanjutan pada data mining. Pada pembahasan ini Preprocessing Data akan dilakukan dalam 2 tahap, yaitu :\n",
    "<ol>\n",
    "    <li>Stop Word</li>\n",
    "    <li>Cleaning Data</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b192a12",
   "metadata": {},
   "source": [
    "### Install Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d878f9a",
   "metadata": {},
   "source": [
    "Library yang perlu diinstall untuk melakukan preprocesing data ada dua yaitu, nltk dan scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71202cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6295ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d41ddc4",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fd786c",
   "metadata": {},
   "source": [
    "Sesudah install, kita import library yang dibutuhkan (untuk string udah otomatis terinstall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50726a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb6decc",
   "metadata": {},
   "source": [
    "### Melakukan Preproces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b4ed0e",
   "metadata": {},
   "source": [
    "#### 1. Stop Word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca5b3b4",
   "metadata": {},
   "source": [
    "Stop Word adalah tahap untuk menghilangkan kata yang tidak memiliki arti, seperti preposisi, konjungsi, dan lain sebagainya. Contoh kata yang dihilangkan dari Stop Word adalah yang, di, ke, dan lainnya. Tanpa perlu berlama-lama mari langsung kepada tahap kodingnya, pertama tama kita load data yang sudah kita crawling tadi. Karena tadi hasil yang saya save dengan nama **crawlingpta.csv** maka pada saat load dengan pandas yang saya tuju adalah file **crawlingpta.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "952ad2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "jurnal = pd.read_csv('crawlingpta.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb037090",
   "metadata": {},
   "source": [
    "Sesudah meload data selanjutnya memilih kolom yang ingin di proses, disini saya akan memproses kolom **abstraksi**, dan pada kolom itu juga saya akan menghilangkan angka yang akan mengganggu. Tahap ini juga termasuk dalam bagian Cleaning Data, tahap ini saya lakukan di awal karena kalau udah masuk ke stop word akan susah di proses. Untuk melakukannya saya buat function yang bernama **remove_number** dan di function ini akan mengembalikan nilai berupa text dimana jika ada angka akan dihapus, dan ketika memanggil kolom dikasih apply dan memanggil functionnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78304335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Mayarakat relawan Indonesia (MRI) Surabaya ter...\n",
       "1      Identifikasi atribut pejalan kaki merupakan sa...\n",
       "2      Skripsi ini bertujuan untuk menganalisis penti...\n",
       "3      \\n\\nTujuan utama dari penelitian ini adalah un...\n",
       "4      Penelitian ini bertujuan untuk dapat mengetahu...\n",
       "                             ...                        \n",
       "847    Penelitian ini bertujuan untuk mengetahui peng...\n",
       "848    Tujuan penelitian ini adalah untuk mengetahui ...\n",
       "849     \\nJenis penelitian ini merupakan penelitian e...\n",
       "850    Ach. Fatahillah, NIM  Program Studi Sosiologi,...\n",
       "851    \\nBayu Krisnatama, “Analisis Pendapatan dan Da...\n",
       "Name: Abstraksi, Length: 852, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_number(text):\n",
    "    return  re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "pre_abstrak = jurnal['Abstraksi'].apply(remove_number)\n",
    "pre_abstrak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a340a",
   "metadata": {},
   "source": [
    "Kemudian langkah sebelum memasuki stop word adalah harus tokenize kalimat dahulu, tokenize adalah proses untuk membagi kalimat ke dalam bagian bagian tertentu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5da2e50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [Mayarakat, relawan, Indonesia, (, MRI, ), Sur...\n",
       "1      [Identifikasi, atribut, pejalan, kaki, merupak...\n",
       "2      [Skripsi, ini, bertujuan, untuk, menganalisis,...\n",
       "3      [Tujuan, utama, dari, penelitian, ini, adalah,...\n",
       "4      [Penelitian, ini, bertujuan, untuk, dapat, men...\n",
       "                             ...                        \n",
       "847    [Penelitian, ini, bertujuan, untuk, mengetahui...\n",
       "848    [Tujuan, penelitian, ini, adalah, untuk, menge...\n",
       "849    [Jenis, penelitian, ini, merupakan, penelitian...\n",
       "850    [Ach, ., Fatahillah, ,, NIM, Program, Studi, S...\n",
       "851    [Bayu, Krisnatama, ,, “, Analisis, Pendapatan,...\n",
       "Name: Abstraksi, Length: 852, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens = pre_abstrak.apply(word_tokenize)\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e68c57",
   "metadata": {},
   "source": [
    "Langkah selanjutnya adalah Stop Word. Karena disini saya menggunakan nltk maka harus menentukan dahulu bahasa yang digunakan untuk menentukan bahasa menggunakan **stopwords.words('indonesian')**. Kemudian jika dirasa list stop word masih ada yang kurang maka kita bisa menambahkan sendiri dengan cara membuat list kata yang tidak ada di stop word kemudian kita extend dengan list yang kita buat sendiri **stop_words.extend(list)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340db24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('indonesian')\n",
    "list = ['a','aajaran','aanslag','aatau','ah','abstak','abstrack','abstract','abstrak','z']\n",
    "stop_words.extend(list)\n",
    "after = [[w for w in temp if w not in stop_words] for temp in word_tokens]\n",
    "for i in after:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c451f3",
   "metadata": {},
   "source": [
    "Untuk logika pada saat stop word sendiri sebagai berikut. Pertama kita set bahasa stop words yang digunakan yaitu **indonesian**. Jika ada list stop words yang tidak ada pada stop words yang disediakan oleh nltk, kita bisa menambahkannya dengan cara membuat list kata yang mau dihilangkan kemudian pada stop wordsnya di extend dengan list yang menyimpan list kata yang ingin dihapus. Kemudian logika untuk perulangannya yaitu ini akan dilooping kata yang ada di dalam nested array, maka kita lakukan 2 kali perulangan. Pertama untuk melooping yg ada di dalam nestednya dengan dikasih logika percabangan jika katanya tidak ada pada list stop wordsnya maka akan masuk, dan yang kedua untuk menentukan list mana yang akan di looping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c51cb6",
   "metadata": {},
   "source": [
    "#### 2. Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5060086",
   "metadata": {},
   "source": [
    "Cleaning Data adalah proses untuk membersihkan data yang ada menjadi data yang bisa diolah. Data yang dibersihkan seperti missing value atau data kosong, karakter asing, menghilangkan angka, dan lain sebaginaya. Untuk proses penghilangan angka sudah dilakukan ketika memilih tabel **abstraksi**, maka sekarang tinggal menghilangkan karakter asing dan sekawannya. Untuk melakukan itu kita bisa menggunakan library string.punctuation. Dimana ia akan menghilangkan karakter asing yang ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f430bc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clearData = [[w for w in z if w not in string.punctuation and w.isalpha()] for z in after] \n",
    "for i in clearData:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a77b2d1",
   "metadata": {},
   "source": [
    "Logika dari code diatas sama seperti proses stop words dimana dilakukan perulangan nested looping untuk mengecek katanya, jika terdeteksi kata itu ada pada **string.punctuation** maka tidak dimasukan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f153fc",
   "metadata": {},
   "source": [
    "## Pemodelan Dengan LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d725d167",
   "metadata": {},
   "source": [
    "LSA merupakan metode yang memanfaatkan model statistik matematis untuk menganalisa struktur semantik suatu teks. LSA bisa digunakan untuk menilai esai dengan mengkonversikan esai menjadi matriks-matriks yang diberi nilai pada masing-masing term untuk dicari kesamaan dengan term referensi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bd25cb",
   "metadata": {},
   "source": [
    "### Install Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b50b0",
   "metadata": {},
   "source": [
    "Library yang butuh untuk diinstall adalah scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52665061",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a0f2d0",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c0e202",
   "metadata": {},
   "source": [
    "Sesudah insall selanjutnya import TfidfVectorizer dan TruncatedSVD dari library scikit learn atau sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdec99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b888f2",
   "metadata": {},
   "source": [
    "### Membuat Document Term Matrix (DTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b1bb7b",
   "metadata": {},
   "source": [
    "Document Term Matrix adalah matriks matematis yang menggambarkan frekuensi istilah yang muncul dalam kumpulan dokumen, dengan DTM kita bisa dengan mudah untuk menentukan jumlah kata individual untuk setiap dokumen atau untuk semua dokumen. Misalkan untuk mengetahui kata mana yang lebih sering muncul dalam kumpulan dokumen dan menggunakan informasi tersebut untuk menentukan kata mana yang lebih mungkin “mewakili” dokumen tersebut. Nilai dari DTM sendiri menggunakan nilai dari TF-Idf. Beberapa poin penting yang perlu diperhatikan:\n",
    "<ol>\n",
    "    <li>LSA pada umumnya diimplementasikan dengan menggunakan nilai TF-Idf dan tidak dengan Count Vectorizer</li>\n",
    "    <li>Nilai parameter max_feature bergantung pada daya komputasi</li>\n",
    "    <li>Nilai default untuk min_df dan max_df agar program dapat bekerja dengan baik</li>\n",
    "    <li>Bisa menggunakan nilai ngram_range yang berbeda</li>\n",
    "</ol>\n",
    "Setelah mengetahui poin penting, berikut code untuk cari Tf-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b46cbea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(doc):\n",
    "    return doc\n",
    "vect = TfidfVectorizer(tokenizer=dummy, lowercase=False, max_features=1000)\n",
    "vect_text = vect.fit_transform(clearData)\n",
    "vect_text_tranpose = vect_text.transpose()\n",
    "df = pd.DataFrame(vect_text_tranpose.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba00a57",
   "metadata": {},
   "source": [
    "Setelah kita set Tf-idf dari data, kita cek dahulu apakah baris dan kolom sesuai dengan kata dan dokumennya menggunakan fungsi **shape**. Berikut adalah implementasinya dengan 5 nilai Tf-idf kata per dokumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "072f4d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(852, 1000)\n",
      "   0    1         2    3    4         5         6    7    8    9    ...  842  \\\n",
      "0  0.0  0.0  0.000000  0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  ...  0.0   \n",
      "1  0.0  0.0  0.000000  0.0  0.0  0.056172  0.111792  0.0  0.0  0.0  ...  0.0   \n",
      "2  0.0  0.0  0.034202  0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  ...  0.0   \n",
      "3  0.0  0.0  0.000000  0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  ...  0.0   \n",
      "4  0.0  0.0  0.000000  0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  ...  0.0   \n",
      "\n",
      "   843  844  845  846  847  848  849  850  851  \n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 852 columns]\n"
     ]
    }
   ],
   "source": [
    "print(vect_text.shape)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad63bab",
   "metadata": {},
   "source": [
    "Setelah kita cek ukurannya, berikut adalah hasil kata yang paling sering muncul dan kata yang paling jarang muncul berdasarkan nilai Tf-idf diatas. Semakin kecil nilainya maka dia sering digunakan, sedangkan jika besar nilainya maka dia jarang digunakan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93794093",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penelitian shift\n",
      "1.1434616265434774\n",
      "6.650147258823569\n"
     ]
    }
   ],
   "source": [
    "idf=vect.idf_\n",
    "dd=dict(zip(vect.get_feature_names(), idf))\n",
    "l=sorted(dd, key=(dd).get)\n",
    "# print(l)\n",
    "print(l[0],l[-1])\n",
    "print(dd['penelitian'])\n",
    "print(dd['shift'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb6337e",
   "metadata": {},
   "source": [
    "Dapat dilihat bahwa kata penilitian adalah kata paling sering muncul, sedangkan shift adalah kata paling jarang muncul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9459383a",
   "metadata": {},
   "source": [
    "## Proses Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230de99e",
   "metadata": {},
   "source": [
    "Setelah membuat DTM, langkah selanjutnya adalah proses Latent Semantic Analysis (LSA). Namun sebelum melakukan LSA ada tahap untuk pengurungan dimensi agar bisa menemukan topik laten yang menangkap hubungan antara kata dan dokumen, pengurangan dimensi ini bisa dilakukan dengan cara Singular Value Decomposition (SVD) seperti dibawah ini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a97f3",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7ba8b8",
   "metadata": {},
   "source": [
    "Singular Value Decomposition (SVD) adalah teknik pada aljabar linear yang memfaktorkan sembarang matrix menjadi 3 matrix yang berbeda, yaitu:\n",
    "<ul>\n",
    "    <li>U = Matrix kolom ortogonal</li>\n",
    "    <li>V = Matrix baris ortogonal</li>\n",
    "    <li>S = Satu Singular Matrix</li>\n",
    "</ul>\n",
    "Sementara itu ada yang namanya Truncated singular value decomposition (SVD), dimana Truncated SVD merupakan kunci untuk mendapatkan topik dari kumpulan dokumen yang diberikan. Rumus dari Truncated SVD kurang lebih seperti ini $A=U S V^{T}$, dimana :\n",
    "<ul>\n",
    "    <li>A mewakili document-term matrix, dengan nilai berbasis hitungan yang ditetapkan di antara setiap dokumen dan pasangan kata. Matrix tersebut memiliki dimensi n x m, dengan n mewakili jumlah dokumen dan m mewakili jumlah kata.</li>\n",
    "    <li>U mewakili document-topic matrix. Pada dasarnya, nilainya menunjukkan kekuatan hubungan antara setiap dokumen dan topik turunannya. Matriks memiliki n x r dimensi, dengan n mewakili jumlah dokumen dan r mewakili jumlah topik.</li>\n",
    "    <li>S mewakili matriks diagonal yang mengevaluasi \"Strength\" setiap topik dalam kumpulan dokumen. Matrix memiliki r x r dimensi, dengan r mewakili jumlah topik.</li>\n",
    "    <li>V mewakili word-topic matrix. Nilai-nilainya menunjukkan kekuatan asosiasi antara setiap kata dan topik yang diturunkan. Matrix tersebut memiliki dimensi m x r, dengan m mewakili jumlah kata dan r mewakili jumlah topik.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69385036",
   "metadata": {},
   "source": [
    "![Visualisasi Truncated SVD](visualisasi.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a186bafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.22321048 -0.2101906   0.05076316 ... -0.00128485 -0.13422382\n",
      "  -0.03937861]\n",
      " [ 0.13770476 -0.04925036  0.0302596  ... -0.16023675 -0.04645213\n",
      "  -0.03396618]\n",
      " [ 0.05777609 -0.06899392 -0.04774173 ...  0.051956   -0.06244748\n",
      "   0.00590277]\n",
      " ...\n",
      " [ 0.22765412 -0.18006729  0.10508355 ... -0.00507767 -0.07534519\n",
      "   0.00126922]\n",
      " [ 0.18020879 -0.16760855 -0.12071342 ...  0.04173434 -0.02178306\n",
      "   0.02979276]\n",
      " [ 0.15163765 -0.18814718 -0.03595521 ... -0.02328596  0.08594999\n",
      "   0.01027479]]\n",
      "(852, 10)\n"
     ]
    }
   ],
   "source": [
    "lsa_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=10, random_state=42)\n",
    "lsa_top=lsa_model.fit_transform(vect_text)\n",
    "\n",
    "print(lsa_top)\n",
    "print(lsa_top.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f59f4a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 :\n",
      "Topic  0  :  22.321047797254273\n",
      "Topic  1  :  -21.019060233823218\n",
      "Topic  2  :  5.076316304717713\n",
      "Topic  3  :  2.657094937469628\n",
      "Topic  4  :  -12.830867978990417\n",
      "Topic  5  :  -5.031788650287868\n",
      "Topic  6  :  5.673181047889446\n",
      "Topic  7  :  -0.12848547610313046\n",
      "Topic  8  :  -13.422382119489873\n",
      "Topic  9  :  -3.937861323781596\n"
     ]
    }
   ],
   "source": [
    "l=lsa_top[0]\n",
    "print(\"Document 0 :\")\n",
    "for i,topic in enumerate(l):\n",
    "  print(\"Topic \",i,\" : \",topic*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a4600a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1000)\n",
      "[[ 0.01722975  0.02354554  0.02214484 ...  0.01009565  0.01052395\n",
      "   0.01259088]\n",
      " [ 0.0022534  -0.01911612 -0.0278433  ... -0.01461903 -0.01861643\n",
      "  -0.0195887 ]\n",
      " [ 0.00785405 -0.02270544 -0.0220577  ... -0.00424094 -0.02008752\n",
      "  -0.00907963]\n",
      " ...\n",
      " [-0.00597358 -0.00183798  0.00017789 ... -0.00530835  0.01268106\n",
      "  -0.00720262]\n",
      " [-0.01487477 -0.01487835 -0.00061892 ...  0.00745174  0.01732684\n",
      "   0.00834296]\n",
      " [ 0.02119781  0.00959129  0.01182404 ... -0.01914525  0.01066964\n",
      "  -0.00475969]]\n"
     ]
    }
   ],
   "source": [
    "print(lsa_model.components_.shape)\n",
    "print(lsa_model.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a217e",
   "metadata": {},
   "source": [
    "### Mengekstrak Topic dan Term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e178cc3",
   "metadata": {},
   "source": [
    "Setelah dilakukan Truncated Matrix, sekarang kita dapat melakukan ekstrak topik dokumen. Pada percobaan kali ini, dilakukan extrak sebanyak 10 topik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "11457d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "siswa pembelajaran penelitian data media hasil kelas Penelitian uji metode \n",
      "\n",
      "Topic 1: \n",
      "pembelajaran siswa media valid perangkat kelas pengembangan ahli materi model \n",
      "\n",
      "Topic 2: \n",
      "karyawan kinerja kerja berpengaruh signifikan variabel positif nilai perusahaan PT \n",
      "\n",
      "Topic 3: \n",
      "siswa kemampuan kelas berpikir konsep kritis pemecahan tes SMP indikator \n",
      "\n",
      "Topic 4: \n",
      "hukum pidana Nomor siswa Tahun Pasal tindak perbuatan ayat hak \n",
      "\n",
      "Topic 5: \n",
      "garam jagung produk air kualitas kadar produksi daun faktor nilai \n",
      "\n",
      "Topic 6: \n",
      "the of The in and to that anak study novel \n",
      "\n",
      "Topic 7: \n",
      "garam the media of air The in hukum and kadar \n",
      "\n",
      "Topic 8: \n",
      "beli jual the of Desa in The akad Islam siswa \n",
      "\n",
      "Topic 9: \n",
      "perangkat pembelajaran garam LKK RPP the model Lembar Perangkat beli \n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = vect.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(lsa_model.components_):\n",
    "    vocab_comp = zip(vocab, comp)\n",
    "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_words:\n",
    "        print(t[0],end=\" \")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
