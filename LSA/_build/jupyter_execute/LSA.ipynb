{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d28543fd",
   "metadata": {},
   "source": [
    "# Topik Modeling Dengan Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8fa985",
   "metadata": {},
   "source": [
    "topic modeling merupakan suatu pendekatan untuk menganalisis kumpulan dokumen berbentuk teks dan mengelompokkan menjadi beberapa topik. Pendekatan tersebut masuk dalam pendekatan Clustering dalam studi Machine Learning. Adapun tahap-tahapnya yaitu : \n",
    "<ol>\n",
    "    <li>Crawling Data</li>\n",
    "    <li>Preprocessing Data</li>\n",
    "    <li>LSA</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b23cd2",
   "metadata": {},
   "source": [
    "# Crawling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5312c0b9",
   "metadata": {},
   "source": [
    "Crawling data adalah suatu teknik untuk mengumpulkan data secara cepat dengan menggunakan url sebagai target data yang akan dikumpulkan. Untuk mengumpulkan data bisa menggunakan berbagai tools atau library yang ada, salah satunya adalah Scrappy. Scrapy adalah framework dari python yang berspesialis dalam melakukan web scraping dalam sekala besar, untuk menggunakan scrapy pertama kita install dahulu Scrapy dengan menggunakan pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0e1dee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Scrapy in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: parsel>=1.5.0 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Scrapy) (1.6.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Scrapy) (58.1.0)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Scrapy) (2.0.5)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Scrapy) (1.0.4)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Scrapy) (22.0.0)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Scrapy) (5.4.0)\n",
      "Requirement already satisfied: Twisted>=17.9.0 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Scrapy) (22.2.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Scrapy) (36.0.1)\n",
      "Requirement already satisfied: protego>=0.1.15 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Scrapy) (0.2.1)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Scrapy) (1.22.0)\n",
      "Requirement already satisfied: tldextract in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Scrapy) (3.2.0)\n",
      "Requirement already satisfied: service-identity>=16.0.0 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Scrapy) (21.1.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Scrapy) (1.6.2)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Scrapy) (1.1.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Scrapy) (0.4.0)\n",
      "Requirement already satisfied: lxml>=3.5.0 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Scrapy) (4.8.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cryptography>=2.0->Scrapy) (1.15.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from itemloaders>=1.0.1->Scrapy) (0.10.0)\n",
      "Requirement already satisfied: six>=1.6.0 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from parsel>=1.5.0->Scrapy) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from service-identity>=16.0.0->Scrapy) (0.2.8)\n",
      "Requirement already satisfied: pyasn1 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from service-identity>=16.0.0->Scrapy) (0.4.8)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from service-identity>=16.0.0->Scrapy) (21.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Twisted>=17.9.0->Scrapy) (4.1.1)\n",
      "Requirement already satisfied: twisted-iocpsupport<2,>=1.0.2 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Twisted>=17.9.0->Scrapy) (1.0.2)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Twisted>=17.9.0->Scrapy) (15.1.0)\n",
      "Requirement already satisfied: incremental>=21.3.0 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Twisted>=17.9.0->Scrapy) (21.3.0)\n",
      "Requirement already satisfied: Automat>=0.8.0 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Twisted>=17.9.0->Scrapy) (20.2.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Twisted>=17.9.0->Scrapy) (21.0.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tldextract->Scrapy) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tldextract->Scrapy) (3.6.0)\n",
      "Requirement already satisfied: idna in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tldextract->Scrapy) (3.3)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tldextract->Scrapy) (2.27.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cffi>=1.12->cryptography>=2.0->Scrapy) (2.21)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.1.0->tldextract->Scrapy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.1.0->tldextract->Scrapy) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.1.0->tldextract->Scrapy) (1.26.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\ASUS A456UR\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1414a47",
   "metadata": {},
   "source": [
    "Setelah menginstall Scrapy, selanjutnya import library yang dibutuhkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c91e8b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f15b770",
   "metadata": {},
   "source": [
    "Sesudah import library yang dibutuhkan, selanjutnya melakukan tahap scraping. Disini tahap Scrape saya simpan di class QuotesSpider dengan parameter scrapy.spider. Variabel start_urls berfungsi untuk menampung target url, dimana start_url akan mendapatkan data dari tahap looping \"for page in range(1,10)\". Function parse memiliki peran melakukan scrap pada element html mana, sedangkan function parse_detail memiliki peran untuk menargetkan secara spesifik seperti : \n",
    "<ul>\n",
    "    <li>Mengambil text htmlnya atau mengambil Linknya</li>\n",
    "    <li>Membuang elemen yang tidak digunakan</li>\n",
    "    <li>Mereplace kata yang tidak digunakan dengan kata yang ingin digunakan</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00bf6bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = []\n",
    "    def __init__(self):\n",
    "        url = 'https://pta.trunojoyo.ac.id/welcome/index/'\n",
    "        for page in range(1,10):\n",
    "            self.start_urls.append(url+str(page))\n",
    "\n",
    "    def parse(self, response):\n",
    "        for detail in response.css('a.gray.button::attr(href)'): \n",
    "            yield response.follow(detail.get(), callback = self.parse_detail)\n",
    "\n",
    "    def parse_detail(self, response):\n",
    "        for data in response.css('#content_journal > ul > li'):\n",
    "            yield{\n",
    "                'Judul': data.css('div:nth-child(2) > a::text').get(),\n",
    "                'Penulis': data.css('div:nth-child(2) > span::text').get().replace('Penulis : ', ''),\n",
    "                'Dospem 1': data.css('div:nth-child(3) > span::text').get().replace('Dosen Pembimbing I : ', ''),\n",
    "                'Dospem 2': data.css('div:nth-child(4) > span::text').get().replace('Dosen Pembimbing II :', ''),\n",
    "                'Abstraksi': data.css('div:nth-child(2) > p::text').get().replace('\\n\\n|\\n','').replace('ABSTRAK', ''),\n",
    "                'Abstraction': data.css('div:nth-child(4) > p::text').get().replace('\\n\\n|\\n','').replace('ABSTRACT', ''),\n",
    "                'Link Download': data.css('div:nth-child(5) > a:nth-child(1)::attr(href)').get().replace('.pdf-0.jpg', '.pdf'),\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bee6c3",
   "metadata": {},
   "source": [
    "Silahkan save codenya dan buka cmd, pastikan terbuka di folder yang ada file scrapingnya. Kemudian jalankan perintah ini di cmd untuk memproses dan menyimpan ke csv \"scrapy runspider namaFile.py -o namaFileKetikaDiSaveUlang.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5277f1e6",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8e06a9",
   "metadata": {},
   "source": [
    "Preprocessing Data adalah suatu teknik untuk merubah data mentah atau raw data menajdi informasi yang bersih dan agar bisa digunakan untuk pengolahan lanjutan pada data mining. Pada pembahasan ini Preprocessing Data akan dilakukan dalam 2 tahap, yaitu :\n",
    "<ol>\n",
    "    <li>Stop Word</li>\n",
    "    <li>Cleaning Data</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b4ed0e",
   "metadata": {},
   "source": [
    "## 1. Stop Word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca5b3b4",
   "metadata": {},
   "source": [
    "Stop Word adalah tahap untuk menghilangkan kata yang tidak memiliki arti, seperti preposisi, konjungsi, dan lain sebagainya. Contoh kata yang dihilangkan dari Stop Word adalah yang, di, ke, dan lainnya. Tanpa perlu berlama-lama mari langsung kepada tahap kodingnya, Karena disini saya menggunakan nltk untuk melakukan Stop Word maka kita perlu menginstall nltk terlebih dahulu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70f3dad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.7)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\ASUS A456UR\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.63.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: click in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fd9380",
   "metadata": {},
   "source": [
    "Dan tidak lupa untuk menginstall pandas juga untuk membantu kita dalam mengolah file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abf677f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.22.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\ASUS A456UR\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c23dd7",
   "metadata": {},
   "source": [
    "Sesudah menginstall semua yang dibutuhkan, selanjutnya kita import library yang dibutuhkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50726a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cab298",
   "metadata": {},
   "source": [
    "Sesudah mengimport yang dibutuhkan, selanjutnya kita load data yang sudah kita crawling tadi. Karena tadi hasil yang saya save dengan nama **crawlingpta.csv** maka pada saat load dengan pandas yang saya tuju adalah file **crawlingpta.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "952ad2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "jurnal = pd.read_csv('crawlingpta.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb037090",
   "metadata": {},
   "source": [
    "Sesudah meload data selanjutnya memilih kolom yang ingin di proses, disini saya akan memproses kolom **abstraksi**, dan pada kolom itu juga saya akan menghilangkan angka yang akan mengganggu. Tahap ini juga termasuk dalam bagian Cleaning Data, tahap ini saya lakukan di awal karena kalau udah masuk ke stop word akan susah di proses. Untuk melakukannya saya buat function yang bernama **remove_number** dan di function ini akan mengembalikan nilai berupa text dimana jika ada angka akan dihapus, dan ketika memanggil kolom dikasih apply dan memanggil functionnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78304335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Mayarakat relawan Indonesia (MRI) Surabaya ter...\n",
       "1      Identifikasi atribut pejalan kaki merupakan sa...\n",
       "2      Skripsi ini bertujuan untuk menganalisis penti...\n",
       "3      \\n\\nTujuan utama dari penelitian ini adalah un...\n",
       "4      Penelitian ini bertujuan untuk dapat mengetahu...\n",
       "                             ...                        \n",
       "847    Penelitian ini bertujuan untuk mengetahui peng...\n",
       "848    Tujuan penelitian ini adalah untuk mengetahui ...\n",
       "849     \\nJenis penelitian ini merupakan penelitian e...\n",
       "850    Ach. Fatahillah, NIM  Program Studi Sosiologi,...\n",
       "851    \\nBayu Krisnatama, “Analisis Pendapatan dan Da...\n",
       "Name: Abstraksi, Length: 852, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_number(text):\n",
    "    return  re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "pre_abstrak = jurnal['Abstraksi'].apply(remove_number)\n",
    "pre_abstrak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a340a",
   "metadata": {},
   "source": [
    "Kemudian langkah sebelum memasuki stop word adalah harus tokenize kalimat dahulu, tokenize adalah proses untuk membagi kalimat ke dalam bagian bagian tertentu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5da2e50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [Mayarakat, relawan, Indonesia, (, MRI, ), Sur...\n",
       "1      [Identifikasi, atribut, pejalan, kaki, merupak...\n",
       "2      [Skripsi, ini, bertujuan, untuk, menganalisis,...\n",
       "3      [Tujuan, utama, dari, penelitian, ini, adalah,...\n",
       "4      [Penelitian, ini, bertujuan, untuk, dapat, men...\n",
       "                             ...                        \n",
       "847    [Penelitian, ini, bertujuan, untuk, mengetahui...\n",
       "848    [Tujuan, penelitian, ini, adalah, untuk, menge...\n",
       "849    [Jenis, penelitian, ini, merupakan, penelitian...\n",
       "850    [Ach, ., Fatahillah, ,, NIM, Program, Studi, S...\n",
       "851    [Bayu, Krisnatama, ,, “, Analisis, Pendapatan,...\n",
       "Name: Abstraksi, Length: 852, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens = pre_abstrak.apply(word_tokenize)\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e68c57",
   "metadata": {},
   "source": [
    "Langkah selanjutnya adalah Stop Word. Karena disini saya menggunakan nltk maka harus menentukan dahulu bahasa yang digunakan untuk menentukan bahasa menggunakan **stopwords.words('indonesian')**. Kemudian jika dirasa list stop word masih ada yang kurang maka kita bisa menambahkan sendiri dengan cara membuat list kata yang tidak ada di stop word kemudian kita extend dengan list yang kita buat sendiri **stop_words.extend(list)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "340db24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Mayarakat relawan Indonesia ( MRI ) Surabaya t...\n",
       "1      Identifikasi atribut pejalan kaki salah peneli...\n",
       "2      Skripsi bertujuan menganalisis Stagnansi hubun...\n",
       "3      Tujuan utama penelitian fotografi komunikasi p...\n",
       "4      Penelitian bertujuan pengaruh motivasi kerja m...\n",
       "                             ...                        \n",
       "847    Penelitian bertujuan pengaruh faktor internal ...\n",
       "848    Tujuan penelitian siasat strategi bertahan hid...\n",
       "849    Jenis penelitian penelitian eksplantif pendeka...\n",
       "850    Ach . Fatahillah , NIM Program Studi Sosiologi...\n",
       "851    Bayu Krisnatama , “ Analisis Pendapatan Dampak...\n",
       "Name: Abstraksi, Length: 852, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stop_w(word):\n",
    "    stop_words = stopwords.words('indonesian')\n",
    "    list = ['a','aajaran','aanslag','aatau','ah','abstak','abstrack','abstract','abstrak','z']\n",
    "    stop_words.extend(list)\n",
    "    removed = [w for w in word if w not in stop_words]\n",
    "    cleaned_text=\" \".join(removed)\n",
    "    return cleaned_text\n",
    "\n",
    "after = word_tokens.apply(stop_w)\n",
    "after"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c451f3",
   "metadata": {},
   "source": [
    "Untuk logika pada saat stop word sendiri sebagai berikut. Pertama membuat fungsi untuk di apply dengan variabel yang menyimpan tokens tadi. Kemudian pada fungsinya kita set bahasa stop words yang digunakan yaitu **indonesian**. Jika ada list stop words yang tidak ada pada stop words yang disediakan oleh nltk, kita bisa menambahkannya dengan cara membuat list kata yang mau dihilangkan kemudian pada stop wordsnya di extend dengan list yang menyimpan list kata yang ingin dihapus. Kemudian logika untuk perulangannya yaitu ini akan dilooping pada array dengan perulangan pada parameter fungsi dan dikasih logika percabangan jika katanya tidak ada pada list stop wordsnya maka akan masuk. Dan untuk mengembalikan agar menyatu jadi kalimat dengan cara set string kosongan kemudian join dengan variabe yang menyimpan list tadi dan terakhir di return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c51cb6",
   "metadata": {},
   "source": [
    "## 2. Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5060086",
   "metadata": {},
   "source": [
    "Cleaning Data adalah proses untuk membersihkan data yang ada menjadi data yang bisa diolah. Data yang dibersihkan seperti missing value atau data kosong, karakter asing, menghilangkan angka, dan lain sebaginaya. Untuk proses penghilangan angka sudah dilakukan ketika memilih tabel **abstraksi**, maka sekarang tinggal menghilangkan karakter asing dan sekawannya. Untuk melakukan itu kita bisa menggunakan library string.punctuation. Dimana ia akan menghilangkan karakter asing yang ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f430bc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Mayarakat relawan Indonesia MRI Surabaya terbe...\n",
       "1      Identifikasi atribut pejalan kaki merupakan sa...\n",
       "2      Skripsi ini bertujuan untuk menganalisis penti...\n",
       "3      Tujuan utama dari penelitian ini adalah untuk ...\n",
       "4      Penelitian ini bertujuan untuk dapat mengetahu...\n",
       "                             ...                        \n",
       "847    Penelitian ini bertujuan untuk mengetahui peng...\n",
       "848    Tujuan penelitian ini adalah untuk mengetahui ...\n",
       "849    Jenis penelitian ini merupakan penelitian eksp...\n",
       "850    Ach Fatahillah NIM Program Studi Sosiologi Jur...\n",
       "851    Bayu Krisnatama “ Analisis Pendapatan dan Damp...\n",
       "Name: Abstraksi, Length: 852, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning_data(data):\n",
    "    removed = [d for d in data if d not in string.punctuation]\n",
    "    cleaned_text=\" \".join(removed)\n",
    "    return cleaned_text\n",
    "\n",
    "clearData = word_tokens.apply(cleaning_data)\n",
    "clearData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a77b2d1",
   "metadata": {},
   "source": [
    "Logika dari code diatas sama seperti proses stop words dimana membuat function dahulu kemudian di apply pada variabel yang menyimpan data sebelumnya. Pada fungsinya melakukan perulangan di dalam array dimana jika kata tidak ada dalam string.punctuation maka akan masuk. Kemudian untuk di joinkan kembali dalam kalimat dengan cara string kosong kemudian di joinkan dan yang terakhr di return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f153fc",
   "metadata": {},
   "source": [
    "# Modeling (LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549bebee",
   "metadata": {},
   "source": [
    "LSA merupakan metode yang memanfaatkan model statistik matematis untuk menganalisa struktur semantik suatu teks. LSA bisa digunakan untuk menilai esai dengan mengkonversikan esai menjadi matriks-matriks yang diberi nilai pada masing-masing term untuk dicari kesamaan dengan term referensi. Sebelum masuk ke materi LSA mari kita cari tf idfnya, untuk mencari tf-idf kita bisa menggunakan bantuan dari sklearn. Berikut cara menginstall sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54c9c512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.1.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\ASUS A456UR\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.22.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.8.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\asus a456ur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbcd0a5",
   "metadata": {},
   "source": [
    "Sesudah install selanjutnya import **TfidfVectorizer** yang berada di **sklearn.feature_extraction.text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72b5ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad77cb98",
   "metadata": {},
   "source": [
    "untuk menggunakan **TfidfVectorizer** tinggal memanggilnya saja kemudian untuk parameternya bisa kita set mau max featuresnya berapa. Sesudah memanggil **TfidfVectorizer** selanjutnya kita fit_transform dengan parameter data yang sudah di preprocessing tadi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3eb40d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(852, 1000)\n",
      "  (0, 422)\t0.015911308179129273\n",
      "  (0, 333)\t0.01583399541833317\n",
      "  (0, 461)\t0.03208494300892695\n",
      "  (0, 183)\t0.051860665817638575\n",
      "  (0, 932)\t0.07480148732114233\n",
      "  (0, 821)\t0.09310301216969677\n",
      "  (0, 40)\t0.0507994709591914\n",
      "  (0, 198)\t0.04498288247752366\n",
      "  (0, 149)\t0.05014006457800768\n",
      "  (0, 862)\t0.04184794827559271\n",
      "  (0, 34)\t0.03467927438215133\n",
      "  (0, 835)\t0.03948958164468449\n",
      "  (0, 788)\t0.030132101607632668\n",
      "  (0, 731)\t0.0425163921134138\n",
      "  (0, 913)\t0.024501451855423295\n",
      "  (0, 248)\t0.028612928766161904\n",
      "  (0, 398)\t0.05386451358203226\n",
      "  (0, 802)\t0.028181165193795592\n",
      "  (0, 836)\t0.051496577670779\n",
      "  (0, 503)\t0.04585439937326079\n",
      "  (0, 256)\t0.014078459341314877\n",
      "  (0, 233)\t0.05430805508595041\n",
      "  (0, 853)\t0.051143044303612425\n",
      "  (0, 745)\t0.038218009444350755\n",
      "  (0, 955)\t0.030902634472215428\n",
      "  :\t:\n",
      "  (851, 149)\t0.09680527601568607\n",
      "  (851, 731)\t0.08208627388835532\n",
      "  (851, 913)\t0.02365244071394395\n",
      "  (851, 256)\t0.013590620134635597\n",
      "  (851, 806)\t0.02794402901381703\n",
      "  (851, 591)\t0.02151220062116037\n",
      "  (851, 140)\t0.1810037100705794\n",
      "  (851, 994)\t0.037833281585846375\n",
      "  (851, 527)\t0.029057944120484427\n",
      "  (851, 31)\t0.052426201548775635\n",
      "  (851, 145)\t0.08972542388473194\n",
      "  (851, 28)\t0.0441890443998219\n",
      "  (851, 551)\t0.03502050690247188\n",
      "  (851, 420)\t0.034669691188639024\n",
      "  (851, 171)\t0.01780602682610239\n",
      "  (851, 156)\t0.0636627325120539\n",
      "  (851, 478)\t0.09146915402529479\n",
      "  (851, 596)\t0.015378690679080112\n",
      "  (851, 964)\t0.026018583449898453\n",
      "  (851, 103)\t0.022227933670527705\n",
      "  (851, 282)\t0.03697327045313454\n",
      "  (851, 651)\t0.05064751361705093\n",
      "  (851, 3)\t0.015798533541528638\n",
      "  (851, 996)\t0.060831942047578004\n",
      "  (851, 150)\t0.05154914444749496\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer(max_features=1000)\n",
    "vect_text = vect.fit_transform(clearData)\n",
    "print(vect_text.shape)\n",
    "print(vect_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f53e181",
   "metadata": {},
   "source": [
    "Sesudah mencari tf selanjutnya mencari idfnya, untuk mencari ifg tinggal memanggil **vect.idf_** dan dimasukan kedalam **dict(zip())**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "371c76b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dan cumi\n",
      "1.0249271066704706\n",
      "7.055612366931734\n"
     ]
    }
   ],
   "source": [
    "idf=vect.idf_\n",
    "dd=dict(zip(vect.get_feature_names(), idf))\n",
    "l=sorted(dd, key=(dd).get)\n",
    "# print(l)\n",
    "print(l[0],l[-1])\n",
    "print(dd['dan'])\n",
    "print(dd['cumi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd458826",
   "metadata": {},
   "source": [
    "Berdasarkan data diatas, __dan__ merupakan kata yang paling sering muncul dan __cumi__ merupakan kata yang jarang muncul. Sesudah mencari tf-idf, selanjutnya kita masuk ke tahap LSAnya. Untuk LSA disiini saya menggunakan bantuan dari modulnya sklearn yaitu **TruncatedSVD**. Pertama tama di import dahulu modulnya dari **sklearn.decomposition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d590691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa03f64b",
   "metadata": {},
   "source": [
    "Selanjutnya kita cari topik yang sering di temui "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b0a0246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.32850097 -0.11525273  0.08537634 ...  0.04660467 -0.13699466\n",
      "  -0.03435604]\n",
      " [ 0.41426266 -0.0008473   0.03200067 ... -0.11502646 -0.19228477\n",
      "  -0.07440796]\n",
      " [ 0.14212964 -0.10789605 -0.16513744 ...  0.15291507 -0.08790457\n",
      "   0.03674358]\n",
      " ...\n",
      " [ 0.22259268 -0.06224495  0.13290682 ...  0.01505692  0.0036023\n",
      "  -0.02844707]\n",
      " [ 0.22863174 -0.08510094 -0.07263236 ...  0.0778988   0.0147824\n",
      "   0.06282196]\n",
      " [ 0.26335842 -0.12435458 -0.01451849 ... -0.01173583  0.08435525\n",
      "   0.04894106]]\n",
      "(852, 10)\n"
     ]
    }
   ],
   "source": [
    "lsa_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=10, random_state=42)\n",
    "lsa_top=lsa_model.fit_transform(vect_text)\n",
    "\n",
    "print(lsa_top)\n",
    "print(lsa_top.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cd2fe4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 :\n",
      "Topic  0  :  32.85009737129387\n",
      "Topic  1  :  -11.525273206900652\n",
      "Topic  2  :  8.537634208779926\n",
      "Topic  3  :  7.117440562639198\n",
      "Topic  4  :  -5.843891722709371\n",
      "Topic  5  :  -10.397668956938892\n",
      "Topic  6  :  -2.0812046304894216\n",
      "Topic  7  :  4.660467106203514\n",
      "Topic  8  :  -13.699465699089517\n",
      "Topic  9  :  -3.435603799756027\n"
     ]
    }
   ],
   "source": [
    "l=lsa_top[0]\n",
    "print(\"Document 0 :\")\n",
    "for i,topic in enumerate(l):\n",
    "  print(\"Topic \",i,\" : \",topic*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "553d76c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1000)\n",
      "[[ 0.01641444  0.00562712  0.04901609 ...  0.00970115  0.00819344\n",
      "   0.013796  ]\n",
      " [-0.0047456  -0.00224171 -0.03824296 ... -0.00585516 -0.01148761\n",
      "  -0.01600021]\n",
      " [-0.01278524  0.01032144 -0.02227802 ... -0.01294159 -0.02039049\n",
      "  -0.01175068]\n",
      " ...\n",
      " [ 0.00296216  0.01642359 -0.0155569  ...  0.02160859  0.00799122\n",
      "  -0.00253614]\n",
      " [-0.01940096 -0.01363804  0.0106873  ... -0.02721054  0.02123619\n",
      "   0.02662152]\n",
      " [ 0.00671964  0.01222919 -0.00523988 ... -0.02098981  0.00782383\n",
      "  -0.01222619]]\n"
     ]
    }
   ],
   "source": [
    "print(lsa_model.components_.shape)\n",
    "print(lsa_model.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5e98c0",
   "metadata": {},
   "source": [
    "Setelah melalui proses yang panjang didapatkan sebuah list dari kata-kata yang penting dan memiliki makna dari setiap 10 topic yang ditampilkan. Sederhananya dibawah ini ditampilkan 10 kata dalam setiap topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67a34ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "yang dan penelitian dengan ini dalam siswa pembelajaran pada data \n",
      "\n",
      "Topic 1: \n",
      "pembelajaran siswa media perangkat pengembangan kelas rata valid model belajar \n",
      "\n",
      "Topic 2: \n",
      "kinerja karyawan kerja signifikan berpengaruh variabel terhadap positif sebesar pengaruh \n",
      "\n",
      "Topic 3: \n",
      "siswa kemampuan data teknik soal penelitian berpikir kelas kritis konsep \n",
      "\n",
      "Topic 4: \n",
      "undang hukum siswa pidana kerja nomor kinerja karyawan pasal kemampuan \n",
      "\n",
      "Topic 5: \n",
      "garam jagung siswa produk pada kemampuan air rata kadar kualitas \n",
      "\n",
      "Topic 6: \n",
      "the of in this and to study that used is \n",
      "\n",
      "Topic 7: \n",
      "garam media undang anak jagung air kadar hukum terhadap madura \n",
      "\n",
      "Topic 8: \n",
      "jual beli desa halal wisata islam syariah produk garam siswa \n",
      "\n",
      "Topic 9: \n",
      "perangkat pembelajaran garam lkk kerja jagung lembar rpp desa model \n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = vect.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(lsa_model.components_):\n",
    "    vocab_comp = zip(vocab, comp)\n",
    "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_words:\n",
    "        print(t[0],end=\" \")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}